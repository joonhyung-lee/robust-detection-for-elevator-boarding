<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="project page for ">
  <meta property="og:title" content="Robust Detection for Autonomous Elevator Boarding using a Mobile Manipulator" />
  <meta property="og:description" content="details about the paper `robust-detection-for-elevator-boarding`" />
  <meta property="og:url" content="https://joonhyung-lee.github.io/robust-detection-for-elevator-boarding/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="assets/images/fig_overview.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="assets/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Robust Detection for Autonomous Elevator Boarding using a Mobile Manipulator</title>
  <link rel="icon" type="image/x-icon" href="assets/images/rilab.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="assets/css/bulma.min.css">
  <link rel="stylesheet" href="assets/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="assets/css/bulma-slider.min.css">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="assets/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="assets/js/fontawesome.all.min.js"></script>
  <script src="assets/js/bulma-carousel.min.js"></script>
  <script src="assets/js/bulma-slider.min.js"></script>
  <script src="assets/js/index.js"></script>

  <!-- Include MathJax Library -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <!-- model viewer -->
  <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>

  <!-- MathJax Configuration -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$$', '$$']],
        processEscapes: true,
      },
      "HTML-CSS": { availableFonts: ["TeX"] },
      showMathMenu: false,
    });
  </script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Robust Detection for Autonomous Elevator Boarding using a Mobile
              Manipulator</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://seungyounshin.github.io/" target=" _blank">Seungyoun Shin</a><sup>1</sup>,
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/joonhyung-%E2%80%8Dlee-2875a8219/" target="_blank">Joonhyung
                    Lee</a><sup>1</sup>,
            </div>
            <div class="is-size-5 publication-authors">

              <span class="author-block">
                <a href="https://junhyug.github.io/" target="_blank">Junhyug Noh</a><sup>2</sup>,
                <span class="author-block">
                  <a href="https://sites.google.com/view/sungjoon-choi/home?authuser=0" target="_blank">Sungjoon
                    Choi</a><sup>1</sup>
                </span>
            </div>

            <div class="is-size-5 publication-authors" style="text-align: center;">
              <span class="author-block">Korea University<sup>1</sup><br></span>
              <h2></h2>
              <span class="author-block">Ewha Womans University<sup>2</sup><br></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-47634-1_2" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                      <a href="assets/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/joonhyung-lee/robust-detection-for-elevator-boarding" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

                <!-- Video Link -->
                <span class="link-block">
                  <a href="https://github.com/joonhyung-lee/robust-detection-for-elevator-boarding" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <hr>

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="assets/videos/demo.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle" style="text-align: left;">
          Our framework is an approach to enable indoor robots to navigate multi-level structures through elevators. Our system accurately perceives elevator states, pushes buttons, and executes boarding sequences using only image sensors. We address challenges like class imbalance and label dependency in real-world datasets using specialized YOLOv7 training techniques.
        </h2>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Indoor robots are becoming increasingly prevalent across a range of sectors, but the challenge of
              navigating multi-level structures through elevators remains largely uncharted. For a robot to operate
              successfully, it's pivotal to have an accurate perception of elevator states. This paper presents a robust
              robotic system, tailored to interact adeptly with elevators by discerning their status, actuating buttons,
              and boarding seamlessly. Given the inherent issues of class imbalance and limited data, we utilize the
              YOLOv7 model and adopt specific strategies to counteract the potential decline in object detection
              performance. Our method effectively confronts the class imbalance and label dependency observed in
              real-world datasets, Our method effectively confronts the class imbalance and label dependency observed in
              real-world datasets, offering a promising approach to improve indoor robotic navigation systems.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Framework Overview -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Framework</h2>
        <div id="framework">
          <img src="assets/images/fig_procedures.png" alt="Autonomous elevator boarding framework" width="70%" style="display: block; margin: 0 auto;"/>
          <h2 class="content has-text-justified" style="text-align: left;">
            An overview of the autonomous elevator boarding process. The procedure is divided into two main categories: (a) button-pushing operations and (b) elevator boarding, which encompass tasks such as path planning, object detection, and interaction. Our GAEMI robot uses a comprehensive perception system to recognize elevator states and interact with them seamlessly.
          </h2>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Framework Overview -->

  <!-- Robot Description -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Robot Platform</h2>
        
        <!-- 이미지는 두 컬럼으로 배치 -->
        <div class="columns">
          <div class="column is-half">
            <div style="text-align: center;">
              <img src="assets/images/fig_robot.png" alt="GAEMI Robot" style="width: 65%;">
              <figcaption class="content is-small has-text-centered">
                <strong>GAEMI Robot:</strong> A sophisticated mobile manipulator equipped with a 5DoF robotic arm and a ZED camera.
              </figcaption>
            </div>
          </div>
          <div class="column is-half">
            <div style="text-align: center;">
              <img src="assets/images/fig_button_push_demo.png" alt="GAEMI Robot" style="width: 50%;">
              <figcaption class="content is-small has-text-centered">
                <strong>Button Pushing Demo:</strong> The robot navigates to the button position and pushes the button.
              </figcaption>
            </div>
          </div>
        </div>
        
        <!-- 텍스트는 단일 컬럼으로 배치 -->
        <div class="columns">
          <div class="column is-full">
            <div class="content has-text-justified">
              <p>
                <strong>GAEMI</strong> is a sophisticated mobile manipulator equipped with a 5DoF robotic arm and a ZED camera. Its non-holonomic base features a 2D LiDAR sensor for obstacle detection and localization within a mapped environment. Additionally, GAEMI has a forward-facing RGB camera that serves as the primary sensor for our elevator perception system. This setup enables GAEMI to navigate complex indoor environments, detect elevator states, and interact with control panels effectively.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Robot Description -->

  <!-- Module Explanations -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Perception System</h2>
        <div id="module" class="columns">

          <!-- First Column -->
          <div class="column is-half">
            <h2 class="subtitle">Label Superset and Status Perception</h2>
            <div class="item">
              <div style="text-align: center;">
              <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth" style="width: 95%;">
                <thead>
                  <tr>
                    <th>Category</th>
                    <th>Parameters</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Elevator Door</td>
                    <td>Opened, Moving, Closed</td>
                  </tr>
                  <tr>
                    <td>Current Robot Floor</td>
                    <td>B6, B5, ..., B1, 1, ..., 63</td>
                  </tr>
                  <tr>
                    <td>Current Elevator Floor (Outside/Inside)</td>
                    <td>B6, B5, ..., B1, 1, ..., 63</td>
                  </tr>
                  <tr>
                    <td>Current Elevator Direction (Outside/Inside)</td>
                    <td>Up, Down, None</td>
                  </tr>
                  <tr class="has-background-grey-lighter">
                    <td>Elevator Button (Outside/Inside)</td>
                    <td>Up, Down, B6, B5, ..., B1, 1, ..., 63</td>
                  </tr>
                </tbody>
              </table>
              </div>
              <figcaption class="content is-small has-text-centered">
                <strong>Label Superset:</strong> White rows represent labels processed by the indicator detection module, while the gray row indicates labels handled by the button detection module using instance segmentation.
              </figcaption>
              <p>
                  The primary objective of our perception system is to ascertain the elevator's status, including door state, current floor, and location. We defined a comprehensive label superset covering all possible scenarios across diverse sites, enabling the robot to make decisions and navigate intricate environments effectively. This is essential for seamless navigation and interaction with elevator systems.
              </p>
            </div>
          </div>

          <!-- Second Column -->
          <div class="column is-half">
            <h2 class="subtitle">Addressing Class Imbalance & Small Object Detection</h2>
            <div class="item item-video1">
              <figure class="video-container">
                <img src="assets/images/fig_class_imbalance.png" alt="Class Imbalance & Small Object Detection" style="width: 100%;">
                <figcaption class="video-caption" style="text-align: left; margin-left: 50px;">
                  Our augmentation strategy addresses three key challenges in elevator perception through a systematic approach:
                  <ol style="margin-top: 0.5em !important; margin-bottom: 0.5em !important; list-style-type: disc;">
                    <li><strong style="color: #FACB5E">Original Dataset:</strong> We start with the base dataset of elevator scenes.</li>
                    <li><strong style="color: #7D7C78">Patch Augmentation:</strong> By cropping high-resolution sections of original images, we enhance the detection of small indicators like floor numbers and buttons.</li>
                    <li><strong style="color: #0A224B">Patch-Blur Augmentation:</strong> We selectively blur frequent objects (e.g., elevator doors) in the high-resolution images and remove their labels, effectively addressing class imbalance and label dependency issues.</li>
                  </ol>
                  This multi-step approach increases dataset diversity while maintaining detection accuracy for critical elevator components.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
  </section>
  <!-- End of module explanations -->

  <!-- Dataset Section -->
  <section class="hero is-small">
    <div class="hero-body">
    <div class="container">
        <h2 class="title is-3">Datasets</h2>
        
        <!-- 이미지와 텍스트를 포함하는 두 컬럼 -->
        <div class="columns">
          <div class="column is-full">
            <div style="text-align: center;">
              <img src="assets/images/fig_datasets.png" alt="Indicator Dataset" style="width: 50%;">
              <figcaption class="content is-small has-text-centered">
                <strong>(a) Indicator dataset:</strong> Object detection dataset tailored to capture the basic status of an elevator. <br>
                <strong>(b) Button dataset:</strong> Instance segmentation dataset designed to identify points of interaction.
              </figcaption>
        </div>
        </div>
      </div>

        <!-- 단일 컬럼 설명 텍스트 -->
      <div class="columns">
          <div class="column is-full">
            <div class="content has-text-justified">
              <p>
                We developed two complementary datasets for our elevator interaction system. The <strong>(a) Indicator dataset</strong> is tailored to capture the basic status of an elevator, including door state, current floor indicators, and directional signals. This object detection dataset provides the fundamental situational awareness needed for decision-making.
              </p>
              <p>
                The <strong>(b) Button dataset</strong> is an instance segmentation dataset designed to identify precise points of interaction between the robot and the elevator, facilitating successful task execution. This dataset includes pixel-level masks for elevator buttons, enabling the robot to accurately locate and interact with control panels. Together, these datasets provide the comprehensive perception capabilities required for autonomous elevator navigation.
              </p>
        </div>
      </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Dataset Section -->

  <!-- Results -->
  <section class="hero is-small" style="margin-bottom: 35px;">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Results</h2>
        
        <!-- Three column layout for tables -->
        <div class="columns">
          <!-- First column -->
          <div class="column is-one-third">
            <div style="text-align: center;">
              <h3 class="subtitle has-text-centered">COCO-mini Evaluation</h3>
              <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; width: 95%;">
                <thead>
                  <tr>
                    <th>Dataset</th>
                    <th>mAP@0.5</th>
                    <th>mAP@0.95</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>COCO-mini (base)</td>
                    <td>0.014</td>
                    <td>0.007</td>
                  </tr>
                  <tr>
                    <td>COCO-blur</td>
                    <td>0.018</td>
                    <td>0.009</td>
                  </tr>
                  <tr>
                    <td>COCO-cutout</td>
                    <td>0.012</td>
                    <td>0.006</td>
                  </tr>
                </tbody>
              </table>
              <figcaption class="content is-small has-text-centered">
                <strong>COCO-mini Evaluation:</strong> <br> Comparison of mAP scores on COCO-mini variations.
              </figcaption>
            </div>
          </div>
          
          <!-- Second column -->
          <div class="column is-one-third">
            <div style="text-align: center;">
              <h3 class="subtitle has-text-centered">Indicator Dataset</h3>
              <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; width: 95%;">
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>mAP@0.5</th>
                    <th>Status Accuracy</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>YOLOv7</td>
                    <td>0.730</td>
                    <td>0.813</td>
                  </tr>
                  <tr>
                    <td>YOLOv7 + patch</td>
                    <td>0.784</td>
                    <td>0.878</td>
                  </tr>
                  <tr class="has-background-grey-lighter">
                    <td>YOLOv7 + patch + blur</td>
                    <td>0.779</td>
                    <td>0.879</td>
                  </tr>
                </tbody>
              </table>
              <figcaption class="content is-small has-text-centered">
                <strong>Indicator Dataset Performance:</strong> <br> Experimental results on Indicator dataset.
              </figcaption>
            </div>
          </div>
          
          <!-- Third column -->
          <div class="column is-one-third">
            <div style="text-align: center;">
              <h3 class="subtitle has-text-centered">Real-world Results</h3>
              <table class="table is-bordered is-striped is-narrow is-hoverable" style="margin: 0 auto; width: 95%;">
                <thead>
                  <tr>
                    <th>Task</th>
                    <th>Success Rate</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>GOTO BUTTON POSE</td>
                    <td>10/10</td>
                  </tr>
                  <tr>
                    <td>BUTTON PUSHING</td>
                    <td>9/10</td>
                  </tr>
                  <tr>
                    <td>ELEVATOR BOARDING</td>
                    <td>3/10</td>
                  </tr>
                </tbody>
              </table>
              <figcaption class="content is-small has-text-centered">
                <strong>Real-world Experiment Results:</strong> <br> Success rates of three tasks executed by the robot over ten trials each.
              </figcaption>
            </div>
          </div>
        </div>
        
        <!-- Single column for map image -->
        <div class="columns" style="margin-top: 30px;">
          <div class="column is-full">
            <div style="text-align: center;">
              <img src="assets/images/fig_map.png" alt="Occupancy Map" style="width: 40%; margin: 0 auto;">
              <figcaption class="content is-small has-text-centered">
                <strong>Occupancy Map:</strong> This figure illustrates the constructed occupancy map of the 6th floor of Woojung Hall of Informatics at Korea University, which serves as the operational landscape for all our real-world robot experiments.
              </figcaption>
            </div>
          </div>
        </div>
        
        <!-- Single column for explanation text -->
        <div class="columns">
          <div class="column is-full">
            <div class="content has-text-justified" style="margin-top: 0px;">
              <p>
                Our experimental results validate the effectiveness of our approach in addressing label dependency and small object detection challenges. The COCO-mini evaluation demonstrates that our blur augmentation technique outperforms both the baseline and cutout methods. On the Indicator dataset, our patch augmentation significantly improved accuracy (mAP@0.5: 0.784, Status Accuracy: 0.878) compared to the baseline, while adding blur augmentation maintained high accuracy while addressing class imbalance.
              </p>
              <p>
                In real-world robot operations conducted at Korea University, we evaluated three critical tasks with varying success rates: 100% for navigation, 90% for button pushing, and 30% for elevator boarding over ten trials each. The lower success rate in elevator boarding indicates areas for future improvement, particularly in handling potential obstructions at the elevator door.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
  </section>
  <!-- End of Results -->

  <!-- Demonstration Section -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Real-World Demonstration</h2>
        <div class="columns is-centered">
          <div class="column is-full">
            <div style="text-align: center;">
              <img src="assets/images/fig_demo.png" alt="System Demonstration" style="width: 90%;">
            </div>
            <figcaption class="content is-small has-text-centered">
              <strong>Demonstration of our integrated robotic system:</strong> A comprehensive illustration of the robot successfully performing tasks within a real-world indoor environment.
            </figcaption>
            <div class="content has-text-justified">
              <p>
                Our final demonstration showcases the GAEMI robot navigating and interacting with elevators in the Woojung Hall of Informatics at Korea University. The system integrates all components - perception, planning, and control - to enable autonomous multi-floor navigation. The robot successfully detects elevator status, navigates to the appropriate position, interacts with buttons, and boards/exits elevators with high reliability. This real-world demonstration validates the practical applicability of our approach for indoor service robots operating in multi-level environments.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End of Demonstration Section -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{shin2023robust,
          title={Robust Detection for Autonomous Elevator Boarding using a Mobile Manipulator},
          author={Shin, Seungyoun and Lee, Joonhyung and Noh, Junhyug and Choi, Sungjoon},
          booktitle={Link Springer},
          year={2023}
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

<script>

  timeoutIds = [];

  function populateDemo(imgs, num) {
    // Get the expanded image
    var expandImg = document.getElementById("expandedImg-" + num);
    // Get the image text
    var imgText = document.getElementById("imgtext-" + num);
    var answer = document.getElementById("answer-" + num);

    // Use the same src in the expanded image as the image being clicked on from the grid
    expandImg.src = imgs.src.replace(".png", ".mp4");
    var video = document.getElementById('demo-video-' + num);
    // or video = $('.video-selector')[0];
    video.pause()
    video.load();
    video.play();
    video.removeAttribute('controls');

    console.log(expandImg.src);
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    var qa = imgs.alt.split("[sep]");
    imgText.innerHTML = qa[0];
    answer.innerHTML = "";
    // Show the container element (hidden with CSS)
    expandImg.parentElement.style.display = "block";
    for (timeoutId of timeoutIds) {
      clearTimeout(timeoutId);
    }

    // NOTE (wliang): Modified from original to read from file instead
    fetch(qa[1])
      .then(response => response.text())
      .then(contents => {
        // Call the processData function and pass the contents as an argument
        typeWriter(contents, 0, qa[0], num);
      })
      .catch(error => console.error('Error reading file:', error));
  }

  function typeWriter(txt, i, q, num) {
    var imgText = document.getElementById("imgtext-" + num);
    var answer = document.getElementById("answer-" + num);
    if (imgText.innerHTML == q) {
      for (let k = 0; k < 5; k++) {
        if (i < txt.length) {
          if (txt.charAt(i) == "\\") {
            answer.innerHTML += "\n";
            i += 1;
          } else {
            answer.innerHTML += txt.charAt(i);
          }
          i++;
        }
      }
      hljs.highlightAll();
      timeoutIds.push(setTimeout(typeWriter, 1, txt, i, q, num));
    }
  }

</script>

</html>